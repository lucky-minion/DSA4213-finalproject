"""
Dataset Loading and Preprocessing - Updated for Simulated FL

This module handles loading and preprocessing of datasets with performance-based allocation.
"""

import torch
from torch.utils.data import DataLoader, Dataset, Subset
from transformers import BertTokenizer
from datasets import load_dataset, load_from_disk
import numpy as np

from config import ModelConfig, DatasetConfig

def load_ag_news_dataset():
    """Load AG News dataset
    
    Returns:
        Dataset: AG News dataset
    """
    dataset = load_from_disk("../ag_news_dataset")
    return dataset

def load_yahoo_dataset():
    """Load Yahoo Answers dataset
    
    Returns:
        Dataset: Yahoo Answers dataset
    """
    dataset = load_from_disk("../yahoo_answers_topics")
    return dataset

def load_trec_dataset():
    """Load TREC dataset
    
    Returns:
        Dataset: TREC dataset
    """
    dataset = load_from_disk("../trec")
    return dataset

# Initialize tokenizer
pretrain_model_path = ModelConfig.PRETRAIN_MODEL_PATH
tokenizer = BertTokenizer.from_pretrained(pretrain_model_path)

def news_preprocess_function(examples):
    """Preprocess function for AG News
    
    Args:
        examples (dict): Batch of examples
        
    Returns:
        dict: Tokenized examples
    """
    return tokenizer(examples["text"], padding="max_length", truncation=True, max_length=128)

def yahoo_preprocess_function(examples):
    """Preprocess function for Yahoo Answers
    
    Args:
        examples (dict): Batch of examples
        
    Returns:
        dict: Tokenized examples
    """
    texts = [f"{title} {content}" for title, content in 
             zip(examples["question_title"], examples["question_content"])]
    return tokenizer(texts, padding="max_length", truncation=True, max_length=128)

def trec_preprocess_function(examples):
    """Preprocess function for TREC
    
    Args:
        examples (dict): Batch of examples
        
    Returns:
        dict: Tokenized examples
    """
    return tokenizer(examples['text'], 
                   padding='max_length',
                   truncation=True,
                   max_length=32)  # Ë∂ÖÁü≠Â∫èÂàó

class NewsDataset(Dataset):
    """PyTorch Dataset for AG News"""
    
    def __init__(self, data):
        """Initialize dataset
        
        Args:
            data: Raw dataset
        """
        self.data = data

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data[idx]
        return {
            "input_ids": torch.tensor(item["input_ids"]),
            "attention_mask": torch.tensor(item["attention_mask"]),
            "labels": torch.tensor(item["label"]),
        }

class YahooDataset(Dataset):
    """PyTorch Dataset for Yahoo Answers"""
    
    def __init__(self, data):
        """Initialize dataset
        
        Args:
            data: Raw dataset
        """
        self.data = data

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data[idx]
        return {
            "input_ids": torch.tensor(item["input_ids"]),
            "attention_mask": torch.tensor(item["attention_mask"]),
            "labels": torch.tensor(item["topic"]),
        }

class TrecDataset(Dataset):
    def __init__(self, data):
        self.data = data

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data[idx]
        return {
            'input_ids': torch.tensor(item['input_ids']),
            'attention_mask': torch.tensor(item['attention_mask']),
            'labels': torch.tensor(item['coarse_label'])  # Á≤óÁ≤íÂ∫¶6ÂàÜÁ±ª
        }

def get_train_dataloaders(batch_size=8, client_id=0, performance_factor=1):
    """Get training DataLoader with performance-based allocation
    
    Args:
        batch_size (int): Batch size
        client_id (int): Client identifier (for logging)
        performance_factor (float): Performance factor (0-1) determining data size
        
    Returns:
        DataLoader: Training data loader
    """
    dataset = load_trec_dataset()
    train_data = dataset["train"]
    
    # Calculate dataset size based on performance factor
    total_size = len(train_data)
    min_size = total_size // 2  # Minimum 1% of data
    max_size = total_size    # Maximum 10% of data
    dataset_size = min(max_size, min_size + int((max_size - min_size) * performance_factor))
    
    print(f"üìä Client {client_id} (performance: {performance_factor:.1f}) allocated {dataset_size} samples")
    
    # Select subset of data
    indices = np.random.choice(total_size, dataset_size, replace=False)
    train_data = train_data.select(indices).shuffle(seed=42)

    # Preprocess and create dataset
    train_data = train_data.map(trec_preprocess_function, batched=True)
    train_dataset = TrecDataset(train_data)

    return DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

def get_test_dataloaders(batch_size=16):
    """Get test DataLoader
    
    Args:
        batch_size (int): Batch size
        
    Returns:
        DataLoader: Test data loader
    """
    dataset = load_trec_dataset()
    test_data = dataset["test"]
    test_data = test_data.map(trec_preprocess_function, batched=True)
    test_dataset = TrecDataset(test_data)
    return DataLoader(test_dataset, batch_size=batch_size)